{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Initialisation\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "from datetime import datetime\n",
    "from Toolshed import Download, Toolbox, VegetationLine, Plotting, Transects\n",
    "import seaborn as sns; sns.set()\n",
    "import ee\n",
    "import geopandas as gpd\n",
    "\n",
    "ee.Initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc676e",
   "metadata": {},
   "source": [
    "## Parameter Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AOI using coordinates of a rectangle\n",
    "\n",
    "# The points represent the corners of a bounding box that go around your site\n",
    "sitename = 'SITENAME'\n",
    "lonmin, lonmax = -2.84869, -2.79878\n",
    "latmin, latmax = 56.32641, 56.39814\n",
    "\n",
    "\n",
    "polygon, point = Toolbox.AOI(lonmin, lonmax, latmin, latmax)\n",
    "# it's recommended to convert the polygon to the smallest rectangle (sides parallel to coordinate axes)       \n",
    "polygon = Toolbox.smallest_rectangle(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Settings\n",
    "\n",
    "# directory where the data will be stored\n",
    "filepath = os.path.join(os.getcwd(), 'Data')\n",
    "if os.path.isdir(filepath) is False:\n",
    "    os.mkdir(filepath)\n",
    "\n",
    "# date range\n",
    "dates = ['2021-05-01', '2021-07-02']\n",
    "if len(dates)>2:\n",
    "    daterange='no'\n",
    "else:\n",
    "    daterange='yes'\n",
    "years = list(Toolbox.daterange(datetime.strptime(dates[0],'%Y-%m-%d'), datetime.strptime(dates[-1],'%Y-%m-%d')))\n",
    "\n",
    "# satellite missions\n",
    "# Input a list of containing any/all of 'L5', 'L8', 'S2'\n",
    "sat_list = ['L5','L8','S2']\n",
    "\n",
    "projection_epsg = 27700 # OSGB 1936\n",
    "image_epsg = 32630 # UTM Zone 30N\n",
    "\n",
    "# put all the inputs into a dictionnary\n",
    "inputs = {'polygon': polygon, 'dates': dates, 'daterange':daterange, 'sat_list': sat_list, 'sitename': sitename, 'filepath':filepath}\n",
    "\n",
    "direc = os.path.join(filepath, sitename)\n",
    "\n",
    "if os.path.isdir(direc) is False:\n",
    "    os.mkdir(direc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e83db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Retrieval\n",
    "\n",
    "# before downloading the images, check how many images are available for your inputs\n",
    "Download.check_images_available(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Download\n",
    "\n",
    "Sat = Toolbox.image_retrieval(inputs)\n",
    "metadata = Toolbox.metadata_collection(sat_list, Sat, filepath, sitename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation Edge Settings\n",
    "\n",
    "BasePath = 'Data/' + sitename + '/Veglines'\n",
    "\n",
    "if os.path.isdir(BasePath) is False:\n",
    "    os.mkdir(BasePath)\n",
    "\n",
    "settings = {\n",
    "    # general parameters:\n",
    "    'cloud_thresh': 0.5,        # threshold on maximum cloud cover\n",
    "    'output_epsg': image_epsg,     # epsg code of spatial reference system desired for the output   \n",
    "    'wetdry':True,              # extract wet-dry boundary as well as veg\n",
    "    # quality control:\n",
    "    'check_detection': True,    # if True, shows each shoreline detection to the user for validation\n",
    "    'adjust_detection': False,  # if True, allows user to adjust the postion of each shoreline by changing the threhold\n",
    "    'save_figure': True,        # if True, saves a figure showing the mapped shoreline for each image\n",
    "    # [ONLY FOR ADVANCED USERS] shoreline detection parameters:\n",
    "    'min_beach_area': 200,     # minimum area (in metres^2) for an object to be labelled as a beach\n",
    "    'buffer_size': 250,         # radius (in metres) for buffer around sandy pixels considered in the shoreline detection\n",
    "    'min_length_sl': 500,       # minimum length (in metres) of shoreline perimeter to be valid\n",
    "    'cloud_mask_issue': False,  # switch this parameter to True if sand pixels are masked (in black) on many images  \n",
    "    # add the inputs defined previously\n",
    "    'inputs': inputs,\n",
    "    'projection_epsg': projection_epsg,\n",
    "    'year_list': years\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af12c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation Edge Reference Line Load-In\n",
    "\n",
    "\"\"\"\n",
    "OPTION 2: Load in coordinates of reference line shapefile and format for use in\n",
    "the veg extraction.\n",
    "\"\"\"\n",
    "\n",
    "#referenceLineShp = os.path.join(inputs['filepath'], sitename,'StAndrews_refLine.shp')\n",
    "referenceLineShp = os.path.join(inputs['filepath'], 'SITE_refLine.shp')\n",
    "referenceLine, ref_epsg = Toolbox.ProcessRefline(referenceLineShp,settings)\n",
    "\n",
    "settings['reference_shoreline'] = referenceLine\n",
    "settings['ref_epsg'] = ref_epsg\n",
    "# Distance to buffer reference line by (this is in metres)\n",
    "settings['max_dist_ref'] = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation Line Extraction\n",
    "\n",
    "\"\"\"\n",
    "OPTION 1: Run extraction tool and return output dates, lines, filenames and \n",
    "image properties.\n",
    "\"\"\"\n",
    "output, output_latlon, output_proj = VegetationLine.extract_veglines(metadata, settings, polygon, dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation Line Extraction Load-In\n",
    "\n",
    "\"\"\"\n",
    "OPTION 2: Load in pre-existing output dates, lines, filenames and image properties.\n",
    "\"\"\"\n",
    "\n",
    "SiteFilepath = os.path.join(inputs['filepath'], sitename)\n",
    "with open(os.path.join(SiteFilepath, sitename + '_output.pkl'), 'rb') as f:\n",
    "    output = pickle.load(f)\n",
    "with open(os.path.join(SiteFilepath, sitename + '_output_latlon.pkl'), 'rb') as f:\n",
    "    output_latlon = pickle.load(f)\n",
    "with open(os.path.join(SiteFilepath, sitename + '_output_proj.pkl'), 'rb') as f:\n",
    "    output_proj = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f936bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate date lines (images taken on the same date by the same satellite)\n",
    "\n",
    "output = Toolbox.remove_duplicates(output) \n",
    "output_latlon = Toolbox.remove_duplicates(output_latlon)\n",
    "output_proj = Toolbox.remove_duplicates(output_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the veglines as shapefiles locally\n",
    "\n",
    "Toolbox.SaveConvShapefiles(output, BasePath, sitename, settings['projection_epsg'])\n",
    "if settings['wetdry'] == True:\n",
    "    Toolbox.SaveConvShapefiles_Water(output, BasePath, sitename, settings['projection_epsg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1781d",
   "metadata": {},
   "source": [
    "## Transect-based Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8928b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shore-normal transects\n",
    "SmoothingWindowSize = 21 \n",
    "NoSmooths = 100\n",
    "TransectSpacing = 10\n",
    "DistanceInland = 100\n",
    "DistanceOffshore = 350\n",
    "\n",
    "BasePath = 'Data/' + sitename + '/veglines'\n",
    "VeglineShp = glob.glob(BasePath+'/*veglines.shp')\n",
    "VeglineGDF = gpd.read_file(VeglineShp[0])\n",
    "WaterlineShp = glob.glob(BasePath+'/*waterlines.shp')\n",
    "WaterlineGDF = gpd.read_file(WaterlineShp[0])\n",
    "\n",
    "# Produce Transects for the reference line\n",
    "TransectSpec =  os.path.join(BasePath, sitename+'_Transects.shp')\n",
    "\n",
    "if os.path.isfile(TransectSpec) is False:\n",
    "    TransectGDF = Transects.ProduceTransects(SmoothingWindowSize, NoSmooths, TransectSpacing, DistanceInland, DistanceOffshore, settings['output_epsg'], sitename, BasePath, referenceLineShp)\n",
    "else:\n",
    "    print('Transects already exist and were loaded')\n",
    "    TransectGDF = gpd.read_file(TransectSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create (or load) intersections with sat and validation lines per transect\n",
    "\n",
    "if os.path.isfile(os.path.join(filepath, sitename, sitename + '_transect_intersects.pkl')):\n",
    "    print('TransectDict exists and was loaded')\n",
    "    with open(os.path.join(filepath , sitename, sitename + '_transect_intersects.pkl'), 'rb') as f:\n",
    "        TransectDict, TransectInterGDF = pickle.load(f)\n",
    "else:\n",
    "    # Get intersections\n",
    "    TransectDict = Transects.GetIntersections(BasePath, TransectGDF, VeglineGDF)\n",
    "    # Save newly intersected transects as shapefile\n",
    "    TransectInterGDF = Transects.SaveIntersections(TransectDict, VeglineGDF, BasePath, sitename, settings['projection_epsg'])\n",
    "    # Repopulate dict with intersection distances along transects normalised to transect midpoints\n",
    "    TransectDict = Transects.CalculateChanges(TransectDict,TransectInterGDF)\n",
    "    if settings['wetdry'] == True:\n",
    "        beachslope = 0.02 # tanBeta StAnd W\n",
    "        # beachslope = 0.04 # tanBeta StAnE\n",
    "        TransectDict = Transects.GetBeachWidth(BasePath, TransectGDF, TransectDict, WaterlineGDF, settings, output, beachslope)  \n",
    "        TransectInterGDF = Transects.SaveWaterIntersections(TransectDict, WaterlineGDF, TransectInterGDF, BasePath, sitename, settings['projection_epsg'])\n",
    "    \n",
    "    with open(os.path.join(filepath , sitename, sitename + '_transect_intersects.pkl'), 'wb') as f:\n",
    "        pickle.dump([TransectDict,TransectInterGDF], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59539ea",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f98108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of veglines against pre-existing ground surveys shapefile\n",
    "\n",
    "# Name of date column in validation shapefile (case sensitive!) \n",
    "DatesCol = 'Date'\n",
    "\n",
    "ValidationShp = './Validation/StAndrews_Veg_Edge_combined_2007_2022_singlepart.shp'\n",
    "validpath = os.path.join(os.getcwd(), 'Data', sitename, 'validation')\n",
    "\n",
    "if os.path.isfile(os.path.join(validpath, sitename + '_valid_dict.pkl')):\n",
    "    print('ValidDict exists and was loaded')\n",
    "    with open(os.path.join(validpath, sitename + '_valid_dict.pkl'), 'rb') as f:\n",
    "        ValidDict = pickle.load(f)\n",
    "else:\n",
    "    ValidDict = Transects.ValidateSatIntersects(sitename, ValidationShp, DatesCol, TransectGDF, TransectDict)\n",
    "    with open(os.path.join(validpath, sitename + '_valid_dict.pkl'), 'wb') as f:\n",
    "        pickle.dump(ValidDict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6baf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify errors between validation and satellite derived lines\n",
    "\n",
    "# add tuples of first and last transect IDs desired for quantifying positional errors on\n",
    "TransectIDList = [(0,10),(50,100)] \n",
    "\n",
    "for TransectIDs in TransectIDList:\n",
    "    Toolbox.QuantifyErrors(sitename, VeglineShp[0],'dates',ValidDict,TransectIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a1956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05dd13b8",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c97308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GIF of satellite images and related shorelines\n",
    "\n",
    "Plotting.SatGIF(metadata,settings,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Plots\n",
    "\n",
    "# add tuples of first and last transect IDs desired for plotting positional errors of\n",
    "TransectIDList = [(0,1741)] \n",
    "\n",
    "for TransectIDs in TransectIDList:\n",
    "    PlotTitle = 'Accuracy of Transects ' + str(TransectIDs[0]) + ' to ' + str(TransectIDs[1])\n",
    "    Plotting.SatViolin(sitename,VeglineShp[0],'dates',ValidDict,TransectIDs, PlotTitle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51882e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Peaks threshold values violin plot\n",
    "sites = [sitename]\n",
    "Plotting.ThresholdViolin(filepath, sites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot of validation vs satellite distances per satellite platform name\n",
    "TransectIDs = (0,len(ValidDict['dates'])) # full site\n",
    "Plotting.PlatformViolin(sitename, VeglineShp, 'satname', ValidDict, TransectIDs, 'Full Site Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69693e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation vs satellite cross-shore distance through time\n",
    "TransectIDs = [0, 10, 50]\n",
    "for TransectID in TransectIDs:\n",
    "    Plotting.ValidTimeseries(sitename, ValidDict, TransectID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satellite cross-shore distance through time\n",
    "TransectIDs = [289,1575]\n",
    "for TransectID in TransectIDs:\n",
    "    DateRange = [0,len(TransectDict['dates'][TransectID])] # integers to decide where in time you want to plot\n",
    "    Plotting.VegTimeseries(sitename, TransectDict, TransectID, DateRange)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
