{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30042e3",
   "metadata": {},
   "source": [
    "# Notebook to drive Vegetation Edge extraction from Satellite Images\n",
    "The programming language we are using is called Python. The code has all been written and this notebook will guide you through modifying the analysis for your own area of interest, and executing the analysis.\n",
    "\n",
    "**To run a code block, click in a cell, hold down shift, and press enter.** An asterisk in square brackets `In [*]:` will appear while the code is being executed, and this will change to a number `In [1]:` when the code is finished. *The order in which you execute the code blocks matters, they must be run in sequence.*\n",
    "\n",
    "Inside blocks of python code there are comments indicated by lines that start with `#`. These lines are not computer code but rather comments providing information about what the code is doing to help you follow along and troubleshoot. \n",
    "\n",
    "Before we get started we need to tell python to import the tools we want to use (these are called modules):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports modules\n",
    "import os, sys, glob, pickle, warnings, matplotlib, ee\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from Toolshed import Download, Toolbox, VegetationLine, Plotting, Transects\n",
    "\n",
    "# Initialise plotting environment and earth engine\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "matplotlib.use('Qt5Agg')\n",
    "plt.ion()\n",
    "sns.set()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc712e56",
   "metadata": {},
   "source": [
    "## Name your project\n",
    "Start by naming your project, then we'll set up a folder structure to store the new data, files and outputs created by the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1384b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE YOUR PROJECT NAME HERE e.g. based on your chosen site\n",
    "sitename = 'SITENAME'\n",
    "\n",
    "# directory where the data will be stored\n",
    "filepath = os.path.join(os.getcwd(), 'Data')\n",
    "if os.path.isdir(filepath) is False:\n",
    "    os.mkdir(filepath)\n",
    "\n",
    "# directory where outputs will be stored\n",
    "direc = os.path.join(filepath, sitename)\n",
    "if os.path.isdir(direc) is False:\n",
    "    os.mkdir(direc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc676e",
   "metadata": {},
   "source": [
    "## Define Area of Interest\n",
    "Area of Interest (AOI) can be defined using a pre-existing shapefile (e.g. by drawing a box within a GIS). If using a shapefile this should only contain one singlepart shape and record (the use of multiple shapes for iteration may be added in the future). Alternatively you can provide the coordinates of the four corners of a bounding box. The shapefile or coordinates should be provided in latitudes and longitudes (i.e. WGS1984, EPSG code 4326)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f2bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shapefile name here if you have one, this should only contain 1 single part shape/record\n",
    "AOIfilename = \"../Musselburgh/AOI.shp\"\n",
    "BLfilename = \"../Musselburgh/Baseline.shp\"\n",
    "\n",
    "# Check if it exists and read the shape if so\n",
    "if Path(AOIfilename).exists():\n",
    "    AOI = gpd.read_file(AOIfilename)\n",
    "    if not AOI.crs == \"epsg:4326\":\n",
    "        sys.exit(\"Wrong Coordinate System\")\n",
    "    \n",
    "    # get minimum bounding box\n",
    "    lonmin, latmin, lonmax, latmax = AOI.bounds.iloc[0]    \n",
    "\n",
    "else:\n",
    "    print(\"No file found\")\n",
    "    # Define AOI using coordinates of a rectangle\n",
    "    # The points represent the corners of a bounding box that go around your site\n",
    "    lonmin, lonmax = -2.84869, -2.79878\n",
    "    latmin, latmax = 56.32641, 56.39814\n",
    "\n",
    "# setup an AOI object\n",
    "UTM_epsg = (int)(Toolbox.get_UTMepsg_from_wgs((latmin+latmax)/2, (lonmin+lonmax)/2))\n",
    "polygon, point = Toolbox.AOI(lonmin, lonmax, latmin, latmax, sitename, UTM_epsg)\n",
    "\n",
    "# it's recommended to convert the polygon to the smallest rectangle (sides parallel to coordinate axes)\n",
    "### why not just call this at the end of the AOI function then?\n",
    "polygon = Toolbox.smallest_rectangle(polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a320c",
   "metadata": {},
   "source": [
    "## Image settings\n",
    "In order to analyse a timeseries of satellite images, we need to define a range of time over which to perform the analysis. We also need to define which satellites we wish to work with, where `L5` is Landsat 5, `L8` is Landsat 8, and `S2` is Sentinel 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Settings\n",
    "# date range\n",
    "StartDate = '2020-01-01'\n",
    "EndDate = '2023-01-01'\n",
    "dates = [StartDate, EndDate]\n",
    "if len(dates)>2:\n",
    "    daterange='no'\n",
    "else:\n",
    "    daterange='yes'\n",
    "years = list(Toolbox.daterange(datetime.strptime(dates[0],'%Y-%m-%d'), datetime.strptime(dates[-1],'%Y-%m-%d')))\n",
    "\n",
    "# satellite missions\n",
    "# Input a list of containing any/all of 'L5', 'L8', 'S2'\n",
    "sat_list = ['L5','L8','S2']\n",
    "\n",
    "projection_epsg = 27700 # OSGB 1936 # THIS WILL ALSO BE OBSELETE?\n",
    "# image_epsg = 32630 # UTM Zone 30N THIS IS NOW OBSELETE AS DEFINED ABOVE\n",
    "\n",
    "# put all the inputs into a dictionnary\n",
    "inputs = {'polygon': polygon, 'dates': dates, 'daterange':daterange, 'sat_list': sat_list, 'sitename': sitename, 'filepath':filepath}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf8e1a",
   "metadata": {},
   "source": [
    "### Check image availability\n",
    "Before we start processing any imagery, we'll check what images are available in the date range specified for the desired platforms. Note that the tool does not download any imagery since it is all processed in Google Earth Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e83db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before downloading the images, check how many images are available for your inputs\n",
    "Download.check_images_available(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Metadata Retrieval\n",
    "Sat = Toolbox.image_retrieval(inputs)\n",
    "metadata = Toolbox.metadata_collection(inputs, Sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation Edge Settings\n",
    "\n",
    "BasePath = 'Data/' + sitename + '/Veglines'\n",
    "\n",
    "if os.path.isdir(BasePath) is False:\n",
    "    os.mkdir(BasePath)\n",
    "\n",
    "settings = {\n",
    "    \n",
    "    # General parameters:\n",
    "    'cloud_thresh': 0.5,        # threshold on maximum cloud cover\n",
    "    'output_epsg': UTM_epsg,  # epsg code of spatial reference system desired for the output   \n",
    "    'wetdry':True,              # extract wet-dry boundary as well as veg\n",
    "    \n",
    "    # Quality control:\n",
    "    'check_detection': False,    # if True, shows each shoreline detection to the user for validation\n",
    "    'adjust_detection': False,  # if True, allows user to adjust the postion of each shoreline by changing the threhold\n",
    "    'save_figure': False,        # if True, saves a figure showing the mapped shoreline for each image\n",
    "    \n",
    "    # [ONLY FOR ADVANCED USERS] shoreline detection parameters:\n",
    "    'min_beach_area': 200,     # minimum area (in metres^2) for an object to be labelled as a beach\n",
    "    'buffer_size': 250,         # radius (in metres) for buffer around sandy pixels considered in the shoreline detection\n",
    "    'min_length_sl': 500,       # minimum length (in metres) of shoreline perimeter to be valid\n",
    "    'cloud_mask_issue': False,  # switch this parameter to True if sand pixels are masked (in black) on many images  \n",
    "    \n",
    "    # add the inputs defined previously\n",
    "    'inputs': inputs,\n",
    "    'projection_epsg': projection_epsg,\n",
    "    'year_list': years\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d61801",
   "metadata": {},
   "source": [
    "### Reference Shoreline\n",
    "Information goes here to explain the reference shoreline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af12c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation Edge Reference Line Load-In\n",
    "referenceLine, ref_epsg = Toolbox.ProcessRefline(BLfilename,settings)\n",
    "DF = gpd.read_file(BLfilename)\n",
    "\n",
    "# update settings with reference line info\n",
    "settings['reference_shoreline'] = referenceLine\n",
    "settings['ref_epsg'] = ref_epsg\n",
    "settings['max_dist_ref'] = 250 # Distance to buffer reference line by (this is in metres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation Line Extraction\n",
    "\n",
    "\"\"\"\n",
    "OPTION 1: Run extraction tool and return output dates, lines, filenames and \n",
    "image properties.\n",
    "\"\"\"\n",
    "\n",
    "clf_model = 'L5L8S2_SAVI_MLPClassifier_Veg.pkl' \n",
    "output, output_latlon, output_proj = VegetationLine.extract_veglines(metadata, settings, polygon, dates, clf_model)\n",
    "\n",
    "### cant run this currently due to lack of tides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation Line Extraction Load-In\n",
    "\n",
    "\"\"\"\n",
    "OPTION 2: Load in pre-existing output dates, lines, filenames and image properties.\n",
    "\"\"\"\n",
    "\n",
    "SiteFilepath = os.path.join(inputs['filepath'], sitename)\n",
    "with open(os.path.join(SiteFilepath, sitename + '_output.pkl'), 'rb') as f:\n",
    "    output = pickle.load(f)\n",
    "with open(os.path.join(SiteFilepath, sitename + '_output_latlon.pkl'), 'rb') as f:\n",
    "    output_latlon = pickle.load(f)\n",
    "with open(os.path.join(SiteFilepath, sitename + '_output_proj.pkl'), 'rb') as f:\n",
    "    output_proj = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f936bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate date lines (images taken on the same date by the same satellite)\n",
    "\n",
    "output = Toolbox.remove_duplicates(output) \n",
    "output_latlon = Toolbox.remove_duplicates(output_latlon)\n",
    "output_proj = Toolbox.remove_duplicates(output_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the veglines as shapefiles locally\n",
    "\n",
    "Toolbox.SaveConvShapefiles(output, BasePath, sitename, settings['projection_epsg'])\n",
    "if settings['wetdry'] == True:\n",
    "    Toolbox.SaveConvShapefiles_Water(output, BasePath, sitename, settings['projection_epsg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1781d",
   "metadata": {},
   "source": [
    "## Transect-based Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8928b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shore-normal transects\n",
    "SmoothingWindowSize = 21 \n",
    "NoSmooths = 100\n",
    "TransectSpacing = 10\n",
    "DistanceInland = 100\n",
    "DistanceOffshore = 350\n",
    "\n",
    "BasePath = 'Data/' + sitename + '/veglines'\n",
    "VeglineShp = glob.glob(BasePath+'/*veglines.shp')\n",
    "VeglineGDF = gpd.read_file(VeglineShp[0])\n",
    "WaterlineShp = glob.glob(BasePath+'/*waterlines.shp')\n",
    "WaterlineGDF = gpd.read_file(WaterlineShp[0])\n",
    "\n",
    "# Produce Transects for the reference line\n",
    "TransectSpec =  os.path.join(BasePath, sitename+'_Transects.shp')\n",
    "\n",
    "if os.path.isfile(TransectSpec) is False:\n",
    "    TransectGDF = Transects.ProduceTransects(SmoothingWindowSize, NoSmooths, TransectSpacing, DistanceInland, DistanceOffshore, settings['output_epsg'], sitename, BasePath, referenceLineShp)\n",
    "else:\n",
    "    print('Transects already exist and were loaded')\n",
    "    TransectGDF = gpd.read_file(TransectSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create (or load) intersections with sat and validation lines per transect\n",
    "\n",
    "if os.path.isfile(os.path.join(filepath, sitename, sitename + '_transect_intersects.pkl')):\n",
    "    print('TransectDict exists and was loaded')\n",
    "    with open(os.path.join(filepath , sitename, sitename + '_transect_intersects.pkl'), 'rb') as f:\n",
    "        TransectDict, TransectInterGDF = pickle.load(f)\n",
    "else:\n",
    "    # Get intersections\n",
    "    TransectDict = Transects.GetIntersections(BasePath, TransectGDF, VeglineGDF)\n",
    "    # Save newly intersected transects as shapefile\n",
    "    TransectInterGDF = Transects.SaveIntersections(TransectDict, VeglineGDF, BasePath, sitename, settings['projection_epsg'])\n",
    "    # Repopulate dict with intersection distances along transects normalised to transect midpoints\n",
    "    TransectDict = Transects.CalculateChanges(TransectDict,TransectInterGDF)\n",
    "    if settings['wetdry'] == True:\n",
    "        beachslope = 0.02 # tanBeta StAnd W\n",
    "        # beachslope = 0.04 # tanBeta StAnE\n",
    "        TransectDict = Transects.GetBeachWidth(BasePath, TransectGDF, TransectDict, WaterlineGDF, settings, output, beachslope)  \n",
    "        TransectInterGDF = Transects.SaveWaterIntersections(TransectDict, WaterlineGDF, TransectInterGDF, BasePath, sitename, settings['projection_epsg'])\n",
    "    \n",
    "    with open(os.path.join(filepath , sitename, sitename + '_transect_intersects.pkl'), 'wb') as f:\n",
    "        pickle.dump([TransectDict,TransectInterGDF], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59539ea",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f98108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of veglines against pre-existing ground surveys shapefile\n",
    "\n",
    "# Name of date column in validation shapefile (case sensitive!) \n",
    "DatesCol = 'Date'\n",
    "\n",
    "ValidationShp = './Validation/StAndrews_Veg_Edge_combined_2007_2022_singlepart.shp'\n",
    "validpath = os.path.join(os.getcwd(), 'Data', sitename, 'validation')\n",
    "\n",
    "if os.path.isfile(os.path.join(validpath, sitename + '_valid_dict.pkl')):\n",
    "    print('ValidDict exists and was loaded')\n",
    "    with open(os.path.join(validpath, sitename + '_valid_dict.pkl'), 'rb') as f:\n",
    "        ValidDict = pickle.load(f)\n",
    "else:\n",
    "    ValidDict = Transects.ValidateSatIntersects(sitename, ValidationShp, DatesCol, TransectGDF, TransectDict)\n",
    "    with open(os.path.join(validpath, sitename + '_valid_dict.pkl'), 'wb') as f:\n",
    "        pickle.dump(ValidDict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6baf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify errors between validation and satellite derived lines\n",
    "\n",
    "# add tuples of first and last transect IDs desired for quantifying positional errors on\n",
    "TransectIDList = [(0,10),(50,100)] \n",
    "\n",
    "for TransectIDs in TransectIDList:\n",
    "    Toolbox.QuantifyErrors(sitename, VeglineShp[0],'dates',ValidDict,TransectIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a1956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05dd13b8",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c97308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GIF of satellite images and related shorelines\n",
    "\n",
    "Plotting.SatGIF(metadata,settings,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Plots\n",
    "\n",
    "# add tuples of first and last transect IDs desired for plotting positional errors of\n",
    "TransectIDList = [(0,1741)] \n",
    "\n",
    "for TransectIDs in TransectIDList:\n",
    "    PlotTitle = 'Accuracy of Transects ' + str(TransectIDs[0]) + ' to ' + str(TransectIDs[1])\n",
    "    Plotting.SatViolin(sitename,VeglineShp[0],'dates',ValidDict,TransectIDs, PlotTitle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51882e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Peaks threshold values violin plot\n",
    "sites = [sitename]\n",
    "Plotting.ThresholdViolin(filepath, sites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot of validation vs satellite distances per satellite platform name\n",
    "TransectIDs = (0,len(ValidDict['dates'])) # full site\n",
    "Plotting.PlatformViolin(sitename, VeglineShp, 'satname', ValidDict, TransectIDs, 'Full Site Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69693e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation vs satellite cross-shore distance through time\n",
    "TransectIDs = [0, 10, 50]\n",
    "for TransectID in TransectIDs:\n",
    "    Plotting.ValidTimeseries(sitename, ValidDict, TransectID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Satellite cross-shore distance through time\n",
    "TransectIDs = [289,1575]\n",
    "for TransectID in TransectIDs:\n",
    "    DateRange = [0,len(TransectDict['dates'][TransectID])] # integers to decide where in time you want to plot\n",
    "    Plotting.VegTimeseries(sitename, TransectDict, TransectID, DateRange)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
